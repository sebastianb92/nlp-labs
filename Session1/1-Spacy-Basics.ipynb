{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRZCz9AP9J1s"
      },
      "source": [
        "<img src=https://upload.wikimedia.org/wikipedia/commons/6/68/Logo_universidad_icesi.svg width=300>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ohf9ibz9PRu"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sebastianb92/nlp-labs/blob/main/Session1/1-Spacy-Basics.ipynb)\n",
        "\n",
        "\n",
        "# Maestría en Inteligencia Artificial  \n",
        "## Procesamiento de Lenguaje natural\n",
        "### Sesión 1 - Práctica\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Integrantes:**  \n",
        "- Johan Sebastian Bonilla  \n",
        "- Edwin Gómez  \n",
        "\n",
        "# Caso práctico: Text Mining – FoodFast Delivery\n",
        "\n",
        "FoodFast Delivery es una app de entrega de comida que recibe montones de comentarios de clientes y repartidores cada día.\n",
        "El equipo de datos quiere automatizar el análisis de estos textos para poder:\n",
        "\n",
        "- detectar las quejas más frecuentes\n",
        "\n",
        "- encontrar segmentos de texto importantes\n",
        "\n",
        "- extraer información clave (productos, ciudades, nombres)\n",
        "\n",
        "- entender tendencias de los comentarios."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAdwOdc3DOE4"
      },
      "source": [
        "**Configurar entorno**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "CMaUNeg_Bs2_"
      },
      "outputs": [],
      "source": [
        "import pkg_resources\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "installed_packages = [package.key for package in pkg_resources.working_set]\n",
        "IN_COLAB = 'google-colab' in installed_packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en6xAfBPa7yT",
        "outputId": "9cc2c7fd-92d3-46c8-8d32-eb276d344f7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==2.1.1 (from -r requirements.txt (line 1))\n",
            "  Using cached pandas-2.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting matplotlib==3.8.0 (from -r requirements.txt (line 2))\n",
            "  Using cached matplotlib-3.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting seaborn==0.12.2 (from -r requirements.txt (line 3))\n",
            "  Using cached seaborn-0.12.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting scikit-learn==1.3.0 (from -r requirements.txt (line 4))\n",
            "  Using cached scikit-learn-1.3.0.tar.gz (7.5 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting statsmodels==0.14.0 (from -r requirements.txt (line 5))\n",
            "  Downloading statsmodels-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting tqdm==4.66.1 (from -r requirements.txt (line 6))\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch==2.2.0 (from -r requirements.txt (line 7))\n",
            "  Downloading torch-2.2.0-cp312-cp312-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Collecting lightning==2.2.0.post0 (from -r requirements.txt (line 8))\n",
            "  Downloading lightning-2.2.0.post0-py3-none-any.whl.metadata (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard==2.16.2 (from -r requirements.txt (line 9))\n",
            "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting bokeh==3.3.4 (from -r requirements.txt (line 10))\n",
            "  Downloading bokeh-3.3.4-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting transformers==4.41.2 (from transformers[torch]==4.41.2->-r requirements.txt (line 11))\n",
            "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.19.1 (from -r requirements.txt (line 12))\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting torchinfo==1.8.0 (from -r requirements.txt (line 13))\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting accelerate==0.30.1 (from -r requirements.txt (line 14))\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting evaluate==0.4.2 (from -r requirements.txt (line 15))\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting sentence-transformers==3.0.1 (from -r requirements.txt (line 16))\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting gradio==4.36.1 (from -r requirements.txt (line 17))\n",
            "  Downloading gradio-4.36.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting ollama==0.2.1 (from -r requirements.txt (line 18))\n",
            "  Downloading ollama-0.2.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting spacy==3.8.7 (from -r requirements.txt (line 19))\n",
            "  Downloading spacy-3.8.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: nltk==3.9.1 in /usr/local/lib/python3.12/dist-packages (from -r requirements.txt (line 20)) (3.9.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.1->-r requirements.txt (line 1)) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.1->-r requirements.txt (line 1)) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.1->-r requirements.txt (line 1)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.1->-r requirements.txt (line 1)) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (1.4.9)\n",
            "Collecting numpy>=1.26.0 (from pandas==2.1.1->-r requirements.txt (line 1))\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (26.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.0->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.0->-r requirements.txt (line 4)) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.0->-r requirements.txt (line 4)) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.3.0->-r requirements.txt (line 4)) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.12/dist-packages (from statsmodels==0.14.0->-r requirements.txt (line 5)) (1.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (3.21.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0->-r requirements.txt (line 7)) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.12/dist-packages (from lightning==2.2.0.post0->-r requirements.txt (line 8)) (6.0.3)\n",
            "Collecting fsspec (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning==2.2.0.post0->-r requirements.txt (line 8))\n",
            "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting packaging>=20.0 (from matplotlib==3.8.0->-r requirements.txt (line 2))\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning==2.2.0.post0->-r requirements.txt (line 8))\n",
            "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting pytorch-lightning (from lightning==2.2.0.post0->-r requirements.txt (line 8))\n",
            "  Downloading pytorch_lightning-2.6.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (1.78.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (3.10.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (5.29.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard==2.16.2->-r requirements.txt (line 9)) (3.1.5)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.12/dist-packages (from bokeh==3.3.4->-r requirements.txt (line 10)) (6.5.1)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.12/dist-packages (from bokeh==3.3.4->-r requirements.txt (line 10)) (2025.11.0)\n",
            "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11))\n",
            "  Downloading huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (2.32.4)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11))\n",
            "  Downloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (0.7.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (18.1.0)\n",
            "Collecting pyarrow-hotfix (from datasets==2.19.1->-r requirements.txt (line 12))\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (0.3.8)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (3.6.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (0.70.16)\n",
            "Collecting fsspec (from torch==2.2.0->-r requirements.txt (line 7))\n",
            "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from datasets==2.19.1->-r requirements.txt (line 12)) (3.13.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate==0.30.1->-r requirements.txt (line 14)) (5.9.5)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==4.36.1->-r requirements.txt (line 17))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (5.5.0)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.12/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.129.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (1.0.0)\n",
            "Collecting gradio-client==1.0.1 (from gradio==4.36.1->-r requirements.txt (line 17))\n",
            "  Downloading gradio_client-1.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.28.1)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.12/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (6.5.2)\n",
            "Collecting markupsafe~=2.0 (from gradio==4.36.1->-r requirements.txt (line 17))\n",
            "  Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (3.11.7)\n",
            "Collecting pillow>=6.2.0 (from matplotlib==3.8.0->-r requirements.txt (line 2))\n",
            "  Downloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (2.12.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.0.22)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.12/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.15.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (2.10.0)\n",
            "Collecting tomlkit==0.12.0 (from gradio==4.36.1->-r requirements.txt (line 17))\n",
            "  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.23.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (2.5.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio==4.36.1->-r requirements.txt (line 17)) (0.40.0)\n",
            "Collecting httpx>=0.24.1 (from gradio==4.36.1->-r requirements.txt (line 17))\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy==3.8.7->-r requirements.txt (line 19)) (0.4.3)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy==3.8.7->-r requirements.txt (line 19))\n",
            "  Downloading langcodes-3.5.1-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk==3.9.1->-r requirements.txt (line 20)) (8.3.1)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio-client==1.0.1->gradio==4.36.1->-r requirements.txt (line 17))\n",
            "  Downloading websockets-11.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.0->-r requirements.txt (line 7)) (12.8.93)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (4.26.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (2.16.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->datasets==2.19.1->-r requirements.txt (line 12)) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (3.11)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio==4.36.1->-r requirements.txt (line 17)) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.36.1->-r requirements.txt (line 17)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.36.1->-r requirements.txt (line 17)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0->gradio==4.36.1->-r requirements.txt (line 17)) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.41.2->transformers[torch]==4.41.2->-r requirements.txt (line 11)) (3.4.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy==3.8.7->-r requirements.txt (line 19)) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy==3.8.7->-r requirements.txt (line 19)) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (0.0.4)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.8.7->-r requirements.txt (line 19)) (0.23.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.8.7->-r requirements.txt (line 19)) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy==3.8.7->-r requirements.txt (line 19)) (7.5.0)\n",
            "Requirement already satisfied: starlette<1.0.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi->gradio==4.36.1->-r requirements.txt (line 17)) (0.52.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.0->-r requirements.txt (line 7)) (1.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==4.36.1->-r requirements.txt (line 17)) (0.30.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy==3.8.7->-r requirements.txt (line 19)) (2.1.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio==4.36.1->-r requirements.txt (line 17)) (0.1.2)\n",
            "Downloading pandas-2.1.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.8.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading statsmodels-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.2.0-cp312-cp312-manylinux1_x86_64.whl (755.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.4/755.4 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.2.0.post0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bokeh-3.3.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio-4.36.1-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ollama-0.2.1-py3-none-any.whl (9.7 kB)\n",
            "Downloading spacy-3.8.7-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.9/33.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.0.1-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.1/318.1 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m830.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\n",
            "Downloading triton-2.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langcodes-3.5.1-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.1/183.1 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-10.4.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Downloading pytorch_lightning-2.6.1-py3-none-any.whl (857 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m857.3/857.3 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-11.0.3-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: scikit-learn\n",
            "  Building wheel for scikit-learn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-learn: filename=scikit_learn-1.3.0-cp312-cp312-linux_x86_64.whl size=10655696 sha256=c98394af4d79b15673b1c1db17ab5531ca7e89cec9e50a4cebd321b99f200fdb\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/e7/56/cae51184980f4c0f7bc63c40ccecad85060b79f19452e19b14\n",
            "Successfully built scikit-learn\n",
            "Installing collected packages: websockets, triton, tqdm, torchinfo, tomlkit, pyarrow-hotfix, pillow, packaging, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, markupsafe, langcodes, fsspec, aiofiles, pandas, nvidia-cusolver-cu12, nvidia-cudnn-cu12, lightning-utilities, huggingface-hub, httpx, torch, tokenizers, tensorboard, statsmodels, scikit-learn, ollama, matplotlib, gradio-client, bokeh, transformers, torchmetrics, seaborn, datasets, accelerate, sentence-transformers, pytorch-lightning, gradio, evaluate, spacy, lightning\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.5.0\n",
            "    Uninstalling triton-3.5.0:\n",
            "      Successfully uninstalled triton-3.5.0\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.3\n",
            "    Uninstalling tqdm-4.67.3:\n",
            "      Successfully uninstalled tqdm-4.67.3\n",
            "  Attempting uninstall: tomlkit\n",
            "    Found existing installation: tomlkit 0.13.3\n",
            "    Uninstalling tomlkit-0.13.3:\n",
            "      Successfully uninstalled tomlkit-0.13.3\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.3.0\n",
            "    Uninstalling pillow-11.3.0:\n",
            "      Successfully uninstalled pillow-11.3.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 26.0\n",
            "    Uninstalling packaging-26.0:\n",
            "      Successfully uninstalled packaging-26.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.8.90\n",
            "    Uninstalling nvidia-nvtx-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
            "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.3\n",
            "    Uninstalling MarkupSafe-3.0.3:\n",
            "      Successfully uninstalled MarkupSafe-3.0.3\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "  Attempting uninstall: aiofiles\n",
            "    Found existing installation: aiofiles 24.1.0\n",
            "    Uninstalling aiofiles-24.1.0:\n",
            "      Successfully uninstalled aiofiles-24.1.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 1.4.1\n",
            "    Uninstalling huggingface_hub-1.4.1:\n",
            "      Successfully uninstalled huggingface_hub-1.4.1\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.9.0+cu128\n",
            "    Uninstalling torch-2.9.0+cu128:\n",
            "      Successfully uninstalled torch-2.9.0+cu128\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.2\n",
            "    Uninstalling tokenizers-0.22.2:\n",
            "      Successfully uninstalled tokenizers-0.22.2\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.19.0\n",
            "    Uninstalling tensorboard-2.19.0:\n",
            "      Successfully uninstalled tensorboard-2.19.0\n",
            "  Attempting uninstall: statsmodels\n",
            "    Found existing installation: statsmodels 0.14.6\n",
            "    Uninstalling statsmodels-0.14.6:\n",
            "      Successfully uninstalled statsmodels-0.14.6\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.6.1\n",
            "    Uninstalling scikit-learn-1.6.1:\n",
            "      Successfully uninstalled scikit-learn-1.6.1\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: gradio-client\n",
            "    Found existing installation: gradio_client 1.14.0\n",
            "    Uninstalling gradio_client-1.14.0:\n",
            "      Successfully uninstalled gradio_client-1.14.0\n",
            "  Attempting uninstall: bokeh\n",
            "    Found existing installation: bokeh 3.7.3\n",
            "    Uninstalling bokeh-3.7.3:\n",
            "      Successfully uninstalled bokeh-3.7.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.13.2\n",
            "    Uninstalling seaborn-0.13.2:\n",
            "      Successfully uninstalled seaborn-0.13.2\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 4.0.0\n",
            "    Uninstalling datasets-4.0.0:\n",
            "      Successfully uninstalled datasets-4.0.0\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 1.12.0\n",
            "    Uninstalling accelerate-1.12.0:\n",
            "      Successfully uninstalled accelerate-1.12.0\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 5.2.2\n",
            "    Uninstalling sentence-transformers-5.2.2:\n",
            "      Successfully uninstalled sentence-transformers-5.2.2\n",
            "  Attempting uninstall: gradio\n",
            "    Found existing installation: gradio 5.50.0\n",
            "    Uninstalling gradio-5.50.0:\n",
            "      Successfully uninstalled gradio-5.50.0\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.11\n",
            "    Uninstalling spacy-3.8.11:\n",
            "      Successfully uninstalled spacy-3.8.11\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.1.1 which is incompatible.\n",
            "opencv-contrib-python 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "torchaudio 2.9.0+cu128 requires torch==2.9.0, but you have torch 2.2.0 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.1.1 which is incompatible.\n",
            "mlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "imbalanced-learn 0.14.1 requires scikit-learn<2,>=1.4.2, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "xarray 2025.12.0 requires pandas>=2.2, but you have pandas 2.1.1 which is incompatible.\n",
            "dataproc-spark-connect 1.0.2 requires tqdm>=4.67, but you have tqdm 4.66.1 which is incompatible.\n",
            "dataproc-spark-connect 1.0.2 requires websockets>=14.0, but you have websockets 11.0.3 which is incompatible.\n",
            "google-adk 1.25.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 11.0.3 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "google-genai 1.63.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-genai 1.63.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 11.0.3 which is incompatible.\n",
            "cuml-cu12 25.10.0 requires scikit-learn>=1.4, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "panel 1.8.7 requires bokeh<3.9.0,>=3.7.0, but you have bokeh 3.3.4 which is incompatible.\n",
            "opencv-python-headless 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "mapclassify 2.10.0 requires scikit-learn>=1.4, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "opencv-python 4.13.0.92 requires numpy>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 2.1.1 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 11.0.3 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires pandas>=2.2, but you have pandas 2.1.1 which is incompatible.\n",
            "tobler 0.13.0 requires tqdm>=4.67, but you have tqdm 4.66.1 which is incompatible.\n",
            "pytensor 2.37.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "esda 2.8.1 requires scikit-learn>=1.4, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "spopt 0.7.0 requires scikit-learn>=1.4.0, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "torchvision 0.24.0+cu128 requires torch==2.9.0, but you have torch 2.2.0 which is incompatible.\n",
            "umap-learn 0.5.11 requires scikit-learn>=1.6, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "libpysal 4.14.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tensorflow 2.19.0 requires tensorboard~=2.19.0, but you have tensorboard 2.16.2 which is incompatible.\n",
            "hdbscan 0.8.41 requires scikit-learn>=1.6, but you have scikit-learn 1.3.0 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed accelerate-0.30.1 aiofiles-23.2.1 bokeh-3.3.4 datasets-2.19.1 evaluate-0.4.2 fsspec-2024.3.1 gradio-4.36.1 gradio-client-1.0.1 httpx-0.27.2 huggingface-hub-0.36.2 langcodes-3.5.1 lightning-2.2.0.post0 lightning-utilities-0.15.2 markupsafe-2.1.5 matplotlib-3.8.0 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvtx-cu12-12.1.105 ollama-0.2.1 packaging-24.2 pandas-2.1.1 pillow-10.4.0 pyarrow-hotfix-0.7 pytorch-lightning-2.6.1 scikit-learn-1.3.0 seaborn-0.12.2 sentence-transformers-3.0.1 spacy-3.8.7 statsmodels-0.14.0 tensorboard-2.16.2 tokenizers-0.19.1 tomlkit-0.12.0 torch-2.2.0 torchinfo-1.8.0 torchmetrics-1.8.2 tqdm-4.66.1 transformers-4.41.2 triton-2.2.0 websockets-11.0.3\n"
          ]
        }
      ],
      "source": [
        "!test \"$IN_COLAB\" = \"True\" && wget -q https://raw.githubusercontent.com/sebastianb92/nlp-labs/main/requirements.txt -O requirements.txt && pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "5tA26tdUE4AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS CELL to perform standard imports:\n",
        "import spacy\n",
        "nlp = spacy.load('es_core_news_sm')"
      ],
      "metadata": {
        "id": "7GievSkoBX-L"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Cargamos documento"
      ],
      "metadata": {
        "id": "9QCoJlnXGoZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Descargar el archivo correctamente\n",
        "!wget -O comments.txt https://raw.githubusercontent.com/sebastianb92/nlp-labs/main/Session1/comments.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIiPnM_tFSBv",
        "outputId": "3576e812-a067-4644-bbcd-a9fe2eb55e68"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-18 00:15:15--  https://raw.githubusercontent.com/sebastianb92/nlp-labs/main/Session1/comments.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2596 (2.5K) [text/plain]\n",
            "Saving to: ‘comments.txt’\n",
            "\n",
            "\rcomments.txt          0%[                    ]       0  --.-KB/s               \rcomments.txt        100%[===================>]   2.54K  --.-KB/s    in 0s      \n",
            "\n",
            "2026-02-18 00:15:15 (37.4 MB/s) - ‘comments.txt’ saved [2596/2596]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('comments.txt', 'r', encoding='utf-8') as file:\n",
        "    comments = [line.strip() for line in file.readlines()]\n",
        "\n",
        "doc = [nlp(comment) for comment in comments]"
      ],
      "metadata": {
        "id": "hK37AFxvGM6s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lon-YJPjGgjb",
        "outputId": "d32ac3dd-6538-4ccc-a6dc-9527251b99aa"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Me llegó tarde la orden 😡 en Bogotá y la pizza estaba fría,\n",
              " Excelente servicio, repartidor muy amable 😊 en Medellín,\n",
              " No me respondieron por cobro doble 💳 en Cali,\n",
              " La app falla 📱 y no permite hacer seguimiento,\n",
              " La hamburguesa 🍔 llegó sin papas y fría]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Cuantos comentarios (tokens) hay en el archivo?"
      ],
      "metadata": {
        "id": "oWAd81rWGu2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQOuVATLGyqn",
        "outputId": "2925b390-1440-4266-9982-797a192250fb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Cuantas oraciones hay en el archivo?"
      ],
      "metadata": {
        "id": "gRgbe9lGHaLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(docs):\n",
        "    print(f\"Comentario {i}: {len(doc)} tokens, {len(list(doc.sents))} oraciones\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4TSXFNAIbvn",
        "outputId": "d0efd17a-a89c-41ae-aa3b-3e6599cb0c64"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comentario 0: 13 tokens, 1 oraciones\n",
            "Comentario 1: 9 tokens, 1 oraciones\n",
            "Comentario 2: 9 tokens, 1 oraciones\n",
            "Comentario 3: 9 tokens, 1 oraciones\n",
            "Comentario 4: 8 tokens, 1 oraciones\n",
            "Comentario 5: 10 tokens, 1 oraciones\n",
            "Comentario 6: 7 tokens, 1 oraciones\n",
            "Comentario 7: 6 tokens, 1 oraciones\n",
            "Comentario 8: 9 tokens, 1 oraciones\n",
            "Comentario 9: 10 tokens, 1 oraciones\n",
            "Comentario 10: 11 tokens, 1 oraciones\n",
            "Comentario 11: 10 tokens, 1 oraciones\n",
            "Comentario 12: 9 tokens, 1 oraciones\n",
            "Comentario 13: 10 tokens, 1 oraciones\n",
            "Comentario 14: 9 tokens, 1 oraciones\n",
            "Comentario 15: 8 tokens, 1 oraciones\n",
            "Comentario 16: 10 tokens, 1 oraciones\n",
            "Comentario 17: 7 tokens, 1 oraciones\n",
            "Comentario 18: 10 tokens, 1 oraciones\n",
            "Comentario 19: 8 tokens, 1 oraciones\n",
            "Comentario 20: 8 tokens, 1 oraciones\n",
            "Comentario 21: 8 tokens, 1 oraciones\n",
            "Comentario 22: 10 tokens, 2 oraciones\n",
            "Comentario 23: 6 tokens, 1 oraciones\n",
            "Comentario 24: 8 tokens, 1 oraciones\n",
            "Comentario 25: 8 tokens, 1 oraciones\n",
            "Comentario 26: 8 tokens, 1 oraciones\n",
            "Comentario 27: 10 tokens, 1 oraciones\n",
            "Comentario 28: 6 tokens, 1 oraciones\n",
            "Comentario 29: 11 tokens, 1 oraciones\n",
            "Comentario 30: 9 tokens, 1 oraciones\n",
            "Comentario 31: 9 tokens, 1 oraciones\n",
            "Comentario 32: 9 tokens, 1 oraciones\n",
            "Comentario 33: 9 tokens, 1 oraciones\n",
            "Comentario 34: 9 tokens, 1 oraciones\n",
            "Comentario 35: 10 tokens, 1 oraciones\n",
            "Comentario 36: 8 tokens, 1 oraciones\n",
            "Comentario 37: 10 tokens, 1 oraciones\n",
            "Comentario 38: 10 tokens, 1 oraciones\n",
            "Comentario 39: 8 tokens, 1 oraciones\n",
            "Comentario 40: 8 tokens, 1 oraciones\n",
            "Comentario 41: 11 tokens, 1 oraciones\n",
            "Comentario 42: 8 tokens, 1 oraciones\n",
            "Comentario 43: 10 tokens, 1 oraciones\n",
            "Comentario 44: 8 tokens, 1 oraciones\n",
            "Comentario 45: 8 tokens, 1 oraciones\n",
            "Comentario 46: 6 tokens, 1 oraciones\n",
            "Comentario 47: 9 tokens, 1 oraciones\n",
            "Comentario 48: 11 tokens, 1 oraciones\n",
            "Comentario 49: 7 tokens, 1 oraciones\n",
            "Comentario 50: 8 tokens, 1 oraciones\n",
            "Comentario 51: 11 tokens, 1 oraciones\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Por cada token en la oración anterior, imprime su text, POS tag, dep tag y lemma"
      ],
      "metadata": {
        "id": "037tMp9iHqBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "rows = []\n",
        "for token in docs[0]:\n",
        "    rows.append([token.text, token.pos_, token.dep_, token.lemma_])\n",
        "\n",
        "pd.DataFrame(rows, columns=[\"Token\",\"POS\",\"Dep\",\"Lemma\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "UsogbB-RHrBZ",
        "outputId": "bfcd6447-e0c8-4282-cae7-55d813588858"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Token    POS     Dep   Lemma\n",
              "0       Me   PRON    iobj      yo\n",
              "1    llegó   VERB    ROOT  llegar\n",
              "2    tarde    ADV  advmod   tarde\n",
              "3       la    DET     det      el\n",
              "4    orden   NOUN   nsubj   orden\n",
              "5        😡   VERB    amod       😡\n",
              "6       en    ADP    case      en\n",
              "7   Bogotá  PROPN     obl  Bogotá\n",
              "8        y  CCONJ      cc       y\n",
              "9       la    DET     det      el\n",
              "10   pizza  PROPN    conj   pizza\n",
              "11  estaba    AUX     cop   estar\n",
              "12    fría    ADJ    conj    frío"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae4ff723-2d36-4831-970b-55abc13ac6cf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Token</th>\n",
              "      <th>POS</th>\n",
              "      <th>Dep</th>\n",
              "      <th>Lemma</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Me</td>\n",
              "      <td>PRON</td>\n",
              "      <td>iobj</td>\n",
              "      <td>yo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>llegó</td>\n",
              "      <td>VERB</td>\n",
              "      <td>ROOT</td>\n",
              "      <td>llegar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tarde</td>\n",
              "      <td>ADV</td>\n",
              "      <td>advmod</td>\n",
              "      <td>tarde</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>la</td>\n",
              "      <td>DET</td>\n",
              "      <td>det</td>\n",
              "      <td>el</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>orden</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>nsubj</td>\n",
              "      <td>orden</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>😡</td>\n",
              "      <td>VERB</td>\n",
              "      <td>amod</td>\n",
              "      <td>😡</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>en</td>\n",
              "      <td>ADP</td>\n",
              "      <td>case</td>\n",
              "      <td>en</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Bogotá</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>obl</td>\n",
              "      <td>Bogotá</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>y</td>\n",
              "      <td>CCONJ</td>\n",
              "      <td>cc</td>\n",
              "      <td>y</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>la</td>\n",
              "      <td>DET</td>\n",
              "      <td>det</td>\n",
              "      <td>el</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>pizza</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>conj</td>\n",
              "      <td>pizza</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>estaba</td>\n",
              "      <td>AUX</td>\n",
              "      <td>cop</td>\n",
              "      <td>estar</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>fría</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>conj</td>\n",
              "      <td>frío</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae4ff723-2d36-4831-970b-55abc13ac6cf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ae4ff723-2d36-4831-970b-55abc13ac6cf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ae4ff723-2d36-4831-970b-55abc13ac6cf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 13,\n  \"fields\": [\n    {\n      \"column\": \"Token\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"estaba\",\n          \"pizza\",\n          \"Me\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"POS\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"AUX\",\n          \"VERB\",\n          \"ADP\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Dep\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"amod\",\n          \"iobj\",\n          \"conj\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lemma\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"estar\",\n          \"pizza\",\n          \"yo\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5) Extracción de frases nominales y entidades"
      ],
      "metadata": {
        "id": "r2Jhm0FUJFuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "    print(\"Entidades:\", [(ent.text, ent.label_) for ent in doc.ents])\n",
        "    print(\"Noun chunks:\", [chunk.text for chunk in doc.noun_chunks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ZRHRl0fI50m",
        "outputId": "e9b4c42a-702c-461b-efe9-23be359cbf0d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entidades: [('Bogotá', 'LOC')]\n",
            "Noun chunks: ['la orden 😡 en Bogotá', 'la pizza']\n",
            "Entidades: [('Medellín', 'LOC')]\n",
            "Noun chunks: ['Excelente servicio, repartidor', '😊', 'Medellín']\n",
            "Entidades: [('Cali', 'LOC')]\n",
            "Noun chunks: ['me', 'cobro doble', 'Cali']\n",
            "Entidades: [('📱', 'ORG')]\n",
            "Noun chunks: ['La app falla 📱', 'seguimiento']\n",
            "Entidades: [('🍔 llegó', 'PER')]\n",
            "Noun chunks: ['La hamburguesa 🍔', 'papas']\n",
            "Entidades: [('llegó', 'PER')]\n",
            "Noun chunks: ['Muy buena experiencia', 'todo rápido y caliente']\n",
            "Entidades: [('Juan', 'PER')]\n",
            "Noun chunks: ['El repartidor', 'Juan']\n",
            "Entidades: []\n",
            "Noun chunks: ['mi pedido 🚫', 'aviso']\n",
            "Entidades: []\n",
            "Noun chunks: ['La pizza']\n",
            "Entidades: []\n",
            "Noun chunks: ['El cobro', 'dos veces', '💳', 'nadie']\n",
            "Entidades: []\n",
            "Noun chunks: ['el pedido', 'varias veces']\n",
            "Entidades: []\n",
            "Noun chunks: ['🥗', 'con mal olor']\n",
            "Entidades: []\n",
            "Noun chunks: ['Buen servicio', 'el repartidor', '👏']\n",
            "Entidades: []\n",
            "Noun chunks: ['la app', 'nada']\n",
            "Entidades: [('Llegó', 'PER'), ('🥤', 'ORG')]\n",
            "Noun chunks: ['el pedido incompleto', 'la bebida 🥤']\n",
            "Entidades: [('llegó', 'PER')]\n",
            "Noun chunks: ['Excelente comida', 'tiempo']\n",
            "Entidades: []\n",
            "Noun chunks: ['El repartidor', 'la dirección 😓']\n",
            "Entidades: []\n",
            "Noun chunks: ['Comida fría ❄️ y poco presentable']\n",
            "Entidades: [('😠', 'MISC')]\n",
            "Noun chunks: ['más de una hora', 'pésimo 😠']\n",
            "Entidades: []\n",
            "Noun chunks: ['Todo', ', muy buen servicio', '⭐']\n",
            "Entidades: [('💬', 'LOC')]\n",
            "Noun chunks: ['El soporte', 'los mensajes']\n",
            "Entidades: []\n",
            "Noun chunks: [', gracias 🙌']\n",
            "Entidades: []\n",
            "Noun chunks: ['La hamburguesa', '🍔']\n",
            "Entidades: []\n",
            "Noun chunks: ['Excelente atención', 'comida', '😋']\n",
            "Entidades: [('llegó', 'PER')]\n",
            "Noun chunks: ['El repartidor']\n",
            "Entidades: []\n",
            "Noun chunks: ['El pedido', '🍅']\n",
            "Entidades: []\n",
            "Noun chunks: ['Muy buena app', 'todo']\n",
            "Entidades: []\n",
            "Noun chunks: ['Cobro incorrecto', 'la tarjeta']\n",
            "Entidades: [('Llegó', 'PER')]\n",
            "Noun chunks: ['un producto diferente']\n",
            "Entidades: [('🍕 llegó fría', 'PER')]\n",
            "Noun chunks: ['La pizza', '🍕', 'fría', 'el repartidor']\n",
            "Entidades: []\n",
            "Noun chunks: ['dudas']\n",
            "Entidades: [('😡', 'LOC')]\n",
            "Noun chunks: ['nadie']\n",
            "Entidades: []\n",
            "Noun chunks: ['Comida buena', 'la entrega']\n",
            "Entidades: [('😠', 'MISC')]\n",
            "Noun chunks: ['El repartidor', 'cambio', ', mala experiencia 😠']\n",
            "Entidades: [('llegó', 'PER')]\n",
            "Noun chunks: ['Excelente servicio', 'todo']\n",
            "Entidades: []\n",
            "Noun chunks: ['Mi pedido', '😞', 'nadie', 'lo']\n",
            "Entidades: []\n",
            "Noun chunks: ['La comida', '😋']\n",
            "Entidades: [('Llegó', 'PER'), ('app', 'ORG')]\n",
            "Noun chunks: ['la app', 'el retraso', '⏰']\n",
            "Entidades: [('😃', 'ORG')]\n",
            "Noun chunks: ['Buen trato', 'repartidor 😃', ', pero la comida']\n",
            "Entidades: []\n",
            "Noun chunks: ['El pedido', 'sin bebida']\n",
            "Entidades: [('😃', 'MISC')]\n",
            "Noun chunks: ['Excelente aplicación, muy fácil de usar 😃']\n",
            "Entidades: []\n",
            "Noun chunks: ['Cobro doble', 'mi tarjeta 💳', ', muy mal servicio 😡']\n",
            "Entidades: []\n",
            "Noun chunks: ['Todo', ', gracias 🙏']\n",
            "Entidades: [('🥤', 'ORG')]\n",
            "Noun chunks: ['La pizza', '🍕', 'la bebida']\n",
            "Entidades: []\n",
            "Noun chunks: ['Muy buena atención 😋 y comida', 'calidad']\n",
            "Entidades: [('llegó', 'PER')]\n",
            "Noun chunks: ['El repartidor', 'la dirección equivocada']\n",
            "Entidades: []\n",
            "Noun chunks: ['previo', 'aviso']\n",
            "Entidades: []\n",
            "Noun chunks: ['Buen servicio', 'problemas', '😊']\n",
            "Entidades: []\n",
            "Noun chunks: ['Comida', '❄️ y entregada tarde']\n",
            "Entidades: [('llegó', 'PER')]\n",
            "Noun chunks: ['Excelente rapidez 🔥,', 'todo']\n",
            "Entidades: [('😤', 'PER')]\n",
            "Noun chunks: ['El repartidor', '😤']\n",
            "Entidades: [('app', 'ORG'), ('😃📱', 'MISC')]\n",
            "Noun chunks: ['Muy buena experiencia', 'la app', '📱']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "patterns = [\n",
        "    [{\"LEMMA\": \"llegar\"}, {\"LOWER\": \"tarde\"}],\n",
        "    [{\"LEMMA\": \"cobrar\"}, {\"LOWER\": \"doble\"}],\n",
        "    [{\"LEMMA\": \"fallar\"}],\n",
        "    [{\"LEMMA\": \"no\"}, {\"POS\": \"VERB\"}]\n",
        "]\n",
        "\n",
        "matcher.add(\"QUEJAS\", patterns)\n"
      ],
      "metadata": {
        "id": "WjgER6_5JNEC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def report_insights(docs):\n",
        "    complaints = []\n",
        "    for doc in docs:\n",
        "        matches = matcher(doc)\n",
        "        if matches:\n",
        "            complaints.append(doc.text)\n",
        "    return complaints\n",
        "\n",
        "print(report_insights(docs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpcByJ2uJROM",
        "outputId": "bbf9fa2b-1af0-457e-c19d-35271c5a13bd"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Me llegó tarde la orden 😡 en Bogotá y la pizza estaba fría', 'La app falla 📱 y no permite hacer seguimiento', 'No funciona la app, no pude pedir nada 😕', 'El repartidor no encontró la dirección 😓 y demoró mucho', 'El soporte no responde los mensajes 💬❌', 'El repartidor fue amable pero llegó tarde 😕', 'El repartidor no tenía cambio, mala experiencia 😠', 'Llegó tarde, la app no notificó el retraso ⏰']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}